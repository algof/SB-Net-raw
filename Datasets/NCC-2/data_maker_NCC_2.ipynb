{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this file is used for making train data (1) and test data (3) from all sensor\n",
    "- The training data will consist of an accumulation of 70% from each sensor\n",
    "- the test data will consist of 30% from each sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('sensor1/sensor1.binetflow')\n",
    "df_2 = pd.read_csv('sensor2/sensor2.binetflow')\n",
    "df_3 = pd.read_csv('sensor3/sensor3.binetflow')\n",
    "\n",
    "df = [df_1, df_2, df_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4895158\t3426610\t1468547\n",
      "5998133\t4198693\t1799439\n",
      "3885792\t2720054\t1165737\n",
      "train_sum 10345357\n",
      "test_sum 4433723\n"
     ]
    }
   ],
   "source": [
    "train_sum = 0\n",
    "test_sum = 0\n",
    "\n",
    "for i in df:\n",
    "    size = len(i)\n",
    "    train_sum += int(size * 0.7)\n",
    "    test_sum += int(size * 0.3)\n",
    "    print(f\"{size}\\t{int(size * 0.7)}\\t{int(size * 0.3)}\")\n",
    "\n",
    "print('train_sum', train_sum)\n",
    "print('test_sum', test_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify the label column\n",
    "def categorize_label(label):\n",
    "    label = str(label).lower()\n",
    "    if 'botnet' in label:\n",
    "        if 'spam' in label:\n",
    "            return 'botnet_spam'\n",
    "        else:\n",
    "            return 'botnet'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "# Apply the function to the 'Label' column\n",
    "for i in df:\n",
    "    i['Label'] = i['Label'].apply(categorize_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make the data simpler, (following my paper) this code will remove the unecessary feature such as:\n",
    "- dTos\n",
    "- sTos\n",
    "- ActivityLabel (only in NCC-2)\n",
    "- BotnetName (only in NCC-2)\n",
    "- SensorId (only in NCC-2)\n",
    "- StartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df)):\n",
    "    df[index] = df[index].drop(\n",
    "        columns=['dTos', 'sTos', 'ActivityLabel', 'BotnetName', 'SensorId', 'StartTime'], \n",
    "        errors='ignore'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = []\n",
    "botnet_df = []\n",
    "botnet_spam_df = []\n",
    "\n",
    "for i in df:\n",
    "    normal_df.append(i[i['Label'] == 'normal'])\n",
    "    botnet_df.append(i[i['Label'] == 'botnet'])\n",
    "    botnet_spam_df.append(i[i['Label'] == 'botnet_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train, normal_test = [], []\n",
    "botnet_train, botnet_test = [], []\n",
    "botnet_spam_train, botnet_spam_test = [], []\n",
    "\n",
    "for a, b, c in zip(normal_df, botnet_df, botnet_spam_df):\n",
    "    temp_train, temp_test = train_test_split(a, test_size=0.3, random_state=42)\n",
    "    normal_train.append(temp_train)\n",
    "    normal_test.append(temp_test)\n",
    "\n",
    "    temp_train, temp_test = train_test_split(b, test_size=0.3, random_state=42)\n",
    "    botnet_train.append(temp_train)\n",
    "    botnet_test.append(temp_test)\n",
    "\n",
    "    temp_train, temp_test = train_test_split(c, test_size=0.3, random_state=42)\n",
    "    botnet_spam_train.append(temp_train)\n",
    "    botnet_spam_test.append(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining training data into one\n",
    "temp_train_df = []\n",
    "for a, b, c in zip(normal_train, botnet_train, botnet_spam_train):\n",
    "    temp_train_df.append(pd.concat([a, b, c], ignore_index=True))\n",
    "train_df = pd.concat(temp_train_df, ignore_index=True)\n",
    "\n",
    "# combining test data for each \n",
    "test_df = []\n",
    "for a, b, c in zip(normal_test, botnet_test, botnet_spam_test):\n",
    "    test_df.append(pd.concat([a, b, c], ignore_index=True))\n",
    "\n",
    "# Shuffle the combined training and testing datasets\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "for index in range(len(test_df)):\n",
    "    test_df[index] = test_df[index].sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahan Algof (24 Oktober 2025 pk 22.56 WIB)\n",
    "train_df = pd.get_dummies(train_df, columns=['Proto'], prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahan Algof (25 Oktober 2025 pk 00.33 WIB)\n",
    "all_states = []\n",
    "\n",
    "for temp_df in df:\n",
    "    all_states.extend(temp_df['State'].unique())\n",
    "    \n",
    "unique_all_states = list(set(all_states))\n",
    "\n",
    "del all_states, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahan Algof (25 Oktober 2025 pk 00.33 WIB)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(unique_all_states)\n",
    "\n",
    "train_df['State'] = le.transform(train_df['State'])\n",
    "\n",
    "del unique_all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('final_dataset/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['1', '2', '5', '9', '13']\n",
    "for a, b in zip(test_df, filenames):\n",
    "    # Tambahan Algof (25 Oktober 2025 pk 00.24 WIB)\n",
    "    a = pd.get_dummies(a, columns=['Proto'], prefix='', prefix_sep='')\n",
    "    a['State'] = le.transform(a['State'])\n",
    "    # ---------------------------------------------\n",
    "    a.to_csv(f'final_dataset/test_{b}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
